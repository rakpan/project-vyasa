# Getting Started Guide

This guide will help you set up and run Project Vyasa on your development machine or NVIDIA DGX system.

## Prerequisites

- **Docker** and **Docker Compose** installed
- **NVIDIA GPU** with CUDA support (for Cortex, Drafter, and Embedder services)
- **Git** for cloning the repository
- **8GB+ RAM** recommended
- **50GB+ disk space** for models and data
- **Project Created via Console**: You must create a project before processing documents (Project-First workflow)

## Step 1: Clone and Navigate

```bash
git clone <repository-url>
cd project-vyasa
```

## Step 2: Bootstrap Secrets (First Time Only)

**Before starting services**, generate secure credentials:

```bash
./scripts/init_vyasa.sh --bootstrap-secrets
```

This creates `deploy/.secrets.env` with auto-generated credentials:
- `ARANGO_ROOT_PASSWORD` (canonical ArangoDB password)
- `CONSOLE_PASSWORD` (NextAuth password)
- `CONSOLE_SECRET` and `NEXTAUTH_SECRET` (session secrets)
- `QDRANT_API_KEY` (optional)

**Note**: [Decision] Credentials are auto-generated by default to prevent hardcoded leaks; override manually in `.env` if needed.

The script checks for existing ArangoDB volumes to prevent lockout from password rotation.

## Step 3: Preflight Check (Required)

**Before starting services**, run the preflight check to validate your environment:

```bash
./scripts/preflight_check.sh
```

This validates:
- ‚úÖ Python import readiness (a2wsgi, nvidia_smi)
- ‚úÖ NVIDIA GB10 superchip detection
- ‚úÖ Unified memory (120GB+ total, 24GB+ available headroom)
- ‚úÖ Knowledge Harvester dataset directory (`/raid/datasets/` writable)
- ‚úÖ Canonical service hostnames (graph:8529, vector:6333, orchestrator:8000)
- ‚úÖ Port availability (30000, 30001, 8529)
- ‚úÖ Expertise configuration (optional warning)

**If any checks fail**: Resolve the issues before proceeding. The script will provide specific error messages and suggestions.

**Expected output**:
```
==========================================
Project Vyasa - Preflight Check
DGX Spark (GB10) Validation
==========================================

Checking NVIDIA GB10 superchip detection... PASS
  Detected: NVIDIA Grace Blackwell
  GPU Count: 1

Checking unified memory (minimum 120GB required)... PASS
  Total Unified Memory: 128GB
  Used Memory: 10GB
  Available Memory: 118GB
  ‚úì 24GB headroom guarantee: PASS (118GB available)

Checking Knowledge Harvester dataset directory... PASS
  Dataset directory: /raid/datasets (writable)

Checking port availability...
  Port 30000 (Brain (Cortex))... AVAILABLE
  Port 30001 (Worker (Cortex))... AVAILABLE
  Port 8529 (Graph (ArangoDB))... AVAILABLE

==========================================
Preflight Check Summary
==========================================
  Passed:  5
  Failed:  0
  Warnings: 0

‚úì Launch Ready
```

## Step 4: Configure Environment

Create your environment configuration file:

```bash
cd deploy
cp .env.example .env
```

Edit `.env` with your configuration:

**Required Settings**:
- `ARANGO_ROOT_PASSWORD` - ArangoDB root password (set a strong password)
- `QDRANT_API_KEY` - Qdrant API key (generate a random string)
- `CONSOLE_PASSWORD` - Console login password
- `NEXTAUTH_SECRET` - NextAuth secret (generate: `openssl rand -base64 32`)
- `HF_TOKEN` - **HuggingFace Hub access token** (required for model downloads; get from https://huggingface.co/settings/tokens)

**Model Configuration** (HuggingFace Hub):
- Models are downloaded from **HuggingFace Hub** on first container start
- **Required**: Set `HF_TOKEN` in your `.env` file (get your token from https://huggingface.co/settings/tokens)
- Model paths use HuggingFace Hub format: `organization/model-name`

**Model Paths** (adjust if using different models):
- `BRAIN_MODEL_PATH` - HuggingFace path to Llama 3.3 70B model (default: `meta-llama/Llama-3.3-70B-Instruct`)
- `WORKER_MODEL_PATH` - HuggingFace path to Worker model (default: `nvidia/Llama-3_3-Nemotron-Super-49B-v1_5`)
- `VISION_MODEL_PATH` - HuggingFace path to Vision model (default: `Qwen/Qwen2-VL-72B-Instruct`)
- `EMBEDDER_MODEL_NAME` - HuggingFace path to embedding model (default: `all-MiniLM-L6-v2`)

**Note**: You can use local paths (e.g., `/path/to/model`) if you've pre-downloaded models, or HuggingFace Hub paths (e.g., `meta-llama/Llama-3.3-70B-Instruct`) to download automatically.

**GPU Assignments**:
- `BRAIN_GPU_IDS` - Comma-separated GPU IDs for Brain (e.g., `0,1`)
- `WORKER_GPU_IDS` - GPU ID for Worker (e.g., `2`)
- `VISION_GPU_IDS` - GPU IDs for Vision (e.g., `3,4`)

**Important**: 
- First-time startup will download models from HuggingFace Hub (this can take time depending on your connection):
  - Brain: ~140GB
  - Worker: ~98GB  
  - Vision: ~144GB
  - Embedder: ~90MB
- If you change model paths, the models will be re-downloaded on first container start
- Some models may require accepting terms on HuggingFace Hub (set `HF_TOKEN` for authenticated downloads)

## Step 5: Start Services

Use the unified stack runner (add `--opik` to include Opik services):

```bash
./scripts/run_stack.sh start
# or: ./scripts/run_stack.sh start --opik
```

The script automatically:
- Creates Docker networks (including `vyasa-net` for Opik if `--opik` is used)
- Creates data directories (no sudo required - Docker handles permissions)
- Checks for port conflicts before starting
- Waits for Opik services to be ready (if `--opik` is used)

Helpful commands:
- Stop services: `./scripts/run_stack.sh stop [--opik]`
- Tail logs: `./scripts/run_stack.sh logs [--opik] [service]`
- Check status: `./scripts/run_stack.sh status [--opik]`

**First Run**: The first time you start the services, Docker will:
1. Pull required images (this may take several minutes)
2. Download ML models from **HuggingFace Hub** on first use (requires `HF_TOKEN` in `.env`):
   - **Brain**: `meta-llama/Llama-3.3-70B-Instruct` (~140GB) from HuggingFace Hub
   - **Worker**: `nvidia/Llama-3_3-Nemotron-Super-49B-v1_5` (~98GB) from HuggingFace Hub
   - **Vision**: `Qwen/Qwen2-VL-72B-Instruct` (~144GB) from HuggingFace Hub
   - **Embedder**: `all-MiniLM-L6-v2` (~90MB) from HuggingFace Hub (via sentence-transformers)
   - **Drafter**: Uses Ollama's model registry (different from HuggingFace)
3. Initialize databases (ArangoDB and Qdrant)
4. Seed initial roles (if running seed script)

**Note**: Model downloads happen automatically when containers start if models aren't already cached. Downloads are managed by:
- **SGLang** (Brain/Worker/Vision): Downloads via `HF_TOKEN` environment variable
- **sentence-transformers** (Embedder): Downloads via HuggingFace Hub (uses `HF_TOKEN` if set)
- **Ollama** (Drafter): Uses Ollama's own model registry (different mechanism)

**Expected Output**: You should see all 9 services starting:
- ‚úÖ `cortex-brain` (Brain - SGLang, Port 30000)
- ‚úÖ `cortex-worker` (Worker - SGLang, Port 30001)
- ‚úÖ `cortex-vision` (Vision - SGLang, Port 30002)
- ‚úÖ `drafter` (Drafter - Ollama, Port 11434)
- ‚úÖ `graph` (Graph - ArangoDB, Port 8529)
- ‚úÖ `console` (Console - Next.js, Port 3000)
- ‚úÖ `embedder` (Embedder - Sentence Transformers, Port 80)
- ‚úÖ `vector` (Vector - Qdrant, Port 6333)
- ‚úÖ `orchestrator` (Orchestrator - Python, Port 8000)

## Step 6: Verify Services

### Quick Verification (Recommended)

Run the deployment verification script to perform an end-to-end integration test:

```bash
./scripts/deploy_verify.sh
```

This script:
1. Creates a temporary project
2. Submits a sample workflow job (raw_text)
3. Waits for completion and verifies triples persisted to ArangoDB
4. Cleans up temporary artifacts

**Expected output**: `[OK]` messages for each step, confirming the system is operational.

### Manual Verification

Check that all containers are running:

```bash
cd deploy
docker compose ps
```

All services should show `Up` status. Expected services:
- ‚úÖ `cortex-brain` (Brain - SGLang, Port 30000)
- ‚úÖ `cortex-worker` (Worker - SGLang, Port 30001)
- ‚úÖ `cortex-vision` (Vision - SGLang, Port 30002)
- ‚úÖ `vyasa-drafter` (Drafter - Ollama, Port 11434)
- ‚úÖ `vyasa-graph` (Graph - ArangoDB, Port 8529)
- ‚úÖ `vyasa-console` (Console - Next.js, Port 3000)
- ‚úÖ `vyasa-embedder` (Embedder - Sentence Transformers, Port 80)
- ‚úÖ `vyasa-qdrant` (Vector - Qdrant, Port 6333)
- ‚úÖ `vyasa-orchestrator` (Orchestrator - Python, Port 8000)

If any service is failing, check logs:

```bash
cd deploy
docker compose logs <service-name>
# Example: docker compose logs cortex-worker
```

**Common issues**:
- **Cortex services failing**: Check GPU availability (`nvidia-smi`) and GPU IDs in `.env`
- **Orchestrator failing**: Check if ArangoDB is healthy (`docker compose logs vyasa-graph`)
- **Console failing**: Check if Orchestrator is healthy (`curl http://localhost:8000/health`)

## Step 7: Access the Console

Open your browser and navigate to:

**http://localhost:3000**

You should see the Project Vyasa Console dashboard. Log in with your `CONSOLE_PASSWORD` (set in `.env`).

## Step 8: Create a Project (Required)

**Project Vyasa uses a Project-First workflow.** All document processing requires a project context.

### 6.1 Navigate to Projects

1. Click **"Projects"** in the navigation or go to `/projects`
2. You should see the Projects home page with a table of existing projects (empty on first run)

### 6.2 Create New Project

1. Click **"New Project"** button
2. Fill in the form:
   - **Title** (required): e.g., "Security Analysis of Web Applications"
   - **Thesis** (required): The core argument or hypothesis
   - **Research Questions** (required): One question per line
   - **Anti-Scope** (optional): Explicitly out-of-scope topics
   - **Target Journal** (optional): e.g., "IEEE Security & Privacy"
3. Click **"Create Project"**
4. You will be redirected to the project workbench (`/projects/[id]`)

**Success**: You now have a project with a unique `project_id`. This ID will be used for all document processing.

## Step 9: Upload Seed Corpus

### 7.1 Navigate to Project Workbench

If you just created a project, you're already on the workbench. Otherwise, click on a project in the projects table.

### 7.2 Upload Files

1. In the **"Seed Corpus"** panel (left column), use the file uploader
2. Drag and drop a PDF file, or click to browse
3. Supported formats: `.pdf`, `.md`, `.txt`, `.json`
4. The file will be uploaded to `/api/proxy/orchestrator/ingest/pdf` with `project_id` automatically included

**Expected**: The file should appear in the "Files" list below the uploader.

## Step 10: Process Documents

### 8.1 Trigger Processing

1. After uploading a file, the system will automatically process it (or you can trigger processing manually)
2. The processing workflow will:
   - Extract text from the PDF
   - Inject project context (Thesis, RQs) into the extraction pipeline
   - Send to Worker (Cartographer) for knowledge graph extraction
   - Tag claims as HIGH/LOW priority based on Research Questions
   - Validate with Critic node
   - Filter by confidence with Vision node
   - Store triples in ArangoDB (linked to `project_id`)

**Expected**: You should see a progress indicator, then a success message.

### 8.2 View Extraction Results

1. Navigate to the **"Processing"** panel (center column)
2. You should see extraction results and graph visualization
3. Nodes and edges are project-scoped (only shows data for the current project)

**Success**: If you see nodes and edges, the extraction pipeline is working! üéâ

## Step 11: Test Search

1. Use the search bar in the Console (if available)
2. Enter a query related to your document (e.g., "security vulnerability")
3. You should see relevant document chunks returned (project-scoped)

**Success**: If search returns results, the vector search pipeline is working! üéâ

## Stopping the System

To gracefully shut down all services:

```bash
./scripts/run_stack.sh stop
# or: ./scripts/run_stack.sh stop --opik
```

To stop and remove volumes (‚ö†Ô∏è **deletes all data**):

```bash
cd deploy
docker compose down -v
```

## Script Reference

| Script | Location | Purpose | When to Use |
|--------|----------|---------|-------------|
| **Preflight Check** | `scripts/preflight_check.sh` | Validates hardware, memory, ports, dataset directory | **Before first startup** |
| **Start/Stop/Logs** | `scripts/run_stack.sh` | Unified compose wrapper (add `--opik` for Opik services) | **Default start/stop** |
| **Operational CLI** | `scripts/vyasa-cli.sh` | Operational utilities (merge nodes, etc.) | **Operations** |
| **Test Runner** | `scripts/run_tests.sh` | Run pytest test suite | **Development** |
| **Mock LLM** | `scripts/run_mock_llm.sh` | Start mock LLM server for testing | **Testing without GPUs** |

### Operational Commands

**Merge graph nodes**:
```bash
./scripts/vyasa-cli.sh merge <job_id> <source_id> <target_id>
```

**Run tests**:
```bash
./scripts/run_tests.sh                    # Unit tests only
./scripts/run_tests.sh --with-integration  # Unit + integration tests
```

**Start mock LLM for testing**:
```bash
./scripts/run_mock_llm.sh
```

## Troubleshooting

### Services Won't Start

**Check GPU availability:**
```bash
nvidia-smi
```

**Check Docker GPU runtime:**
```bash
docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi
```

### Cortex Services Fail (Brain/Worker/Vision)

**Check logs:**
```bash
docker-compose logs cortex-brain
docker-compose logs cortex-worker
docker-compose logs cortex-vision
```

**Common issues:**
- **Model download failed**: 
  - Check internet connection and disk space
  - Verify `HF_TOKEN` is set in `.env` (required for HuggingFace Hub downloads)
  - Some models may require accepting terms on HuggingFace Hub (visit model page and click "Agree")
  - Check container logs: `docker compose logs cortex-brain` (look for HuggingFace download errors)
- GPU not available: Verify `nvidia-smi` works and GPU IDs are correct in `.env`
- Port conflict: 
  - The `run_stack.sh` script automatically detects port conflicts before starting
  - Change `PORT_BRAIN`, `PORT_WORKER`, `PORT_VISION`, or `PORT_DRAFTER` in `.env` if conflicts are detected
  - Common conflict: Port 11434 (drafter) may be in use by another Ollama instance
- GPU reservation conflict: Ensure GPU IDs don't overlap between services

### Console Shows Connection Errors

**Verify service URLs:**
- Check that `ORCHESTRATOR_URL`, `MEMORY_SERVICE_URL`, etc. are correct in `.env`
- Ensure all services are running: `docker-compose ps`

### Orchestrator Fails to Start

**"uvicorn not found" error**:
- This is automatically fixed - the orchestrator installs dependencies from `requirements.txt` on startup
- The `requirements.txt` file is mounted into the container automatically
- If the error persists, check that `requirements.txt` exists at the project root
- Check logs: `./scripts/run_stack.sh logs orchestrator`

**Other orchestrator issues**:
- Ensure ArangoDB is healthy: `curl http://localhost:8529/_api/version`
- Check logs: `./scripts/run_stack.sh logs orchestrator`
- Verify all dependencies are installed (check container logs for import errors)

### Database Initialization Fails

**Reset databases (‚ö†Ô∏è deletes all data):**
```bash
docker-compose down -v
docker-compose up -d
```

### Document Upload Fails with "No active project selected"

**Solution**: You must create a project first. Navigate to `/projects` and click "New Project".

### Extraction Returns Empty Graph

**Check**:
- Project context is being injected (check logs for "Hydrated project context")
- Research Questions are defined in the project
- Document contains relevant content matching the Thesis/RQs

## Next Steps

Once the smoke test passes:

1. **Read the Architecture Docs**: Understand how services interact
   - [System Map](../architecture/system-map.md)
   - [Agent Workflow](../architecture/agent-workflow.md)

2. **Explore the Codebase**:
   - `src/console/` - Next.js frontend
   - `src/orchestrator/` - LangGraph workflows
   - `src/ingestion/` - Knowledge extraction logic
   - `src/project/` - Project Kernel (ProjectConfig, ProjectService)
   - `src/shared/` - Shared schemas and config

3. **Review Decisions**: Understand why we made certain choices
   - [ADR 001: Local Vector DB](../decisions/001-local-vector-db.md)

## Development Workflow

### Making Changes

1. **Frontend changes**: Edit files in `src/console/`
   - Changes hot-reload in development mode
   - Rebuild: `docker-compose build console`

2. **Backend changes**: Edit files in `src/orchestrator/`, `src/ingestion/`
   - Changes require container restart: `docker-compose restart orchestrator`

3. **Configuration changes**: Edit `deploy/.env`
   - Restart affected services: `docker-compose restart <service>`

### Viewing Logs

```bash
# All services
docker-compose logs -f

# Specific service
docker-compose logs -f cortex-brain

# Last 100 lines
docker-compose logs --tail=100 console
```

### Stopping Services

```bash
# Stop all services
docker-compose down

# Stop and remove volumes (‚ö†Ô∏è deletes data)
docker-compose down -v
```

## Service Health Checks

### Manual Health Checks

```bash
# Brain
curl http://localhost:30000/health

# Worker
curl http://localhost:30001/health

# Vision
curl http://localhost:30002/health

# Orchestrator
curl http://localhost:8000/health

# Qdrant
curl http://localhost:6333/health

# ArangoDB
curl http://localhost:8529/_api/version
```

### Console Health

The Console UI includes a health status indicator. Check the settings panel for service connectivity.

## Common Commands Reference

```bash
# Start services
docker-compose up -d

# Stop services
docker-compose down

# View logs
docker-compose logs -f

# Restart a service
docker-compose restart cortex-brain

# Rebuild a service
docker-compose build console

# Check service status
docker-compose ps

# Execute command in container
docker-compose exec orchestrator bash
```

## Getting Help

- **Documentation**: See [docs/README.md](../README.md) for full documentation index
- **Architecture**: Review [System Map](../architecture/system-map.md) for data flows
- **Issues**: Check service logs first: `docker-compose logs <service>`

## Success Criteria

You've successfully set up Project Vyasa when:

‚úÖ All 9 services are running (`docker-compose ps`)  
‚úÖ Console is accessible at http://localhost:3000  
‚úÖ You can create a project via the Console  
‚úÖ You can upload files to a project's seed corpus  
‚úÖ Document processing extracts knowledge graph (nodes and edges)  
‚úÖ Knowledge graph visualization shows project-scoped data  
‚úÖ Search functionality returns relevant results  
‚úÖ Async job system works (submit job, poll status)  

Welcome to Project Vyasa! üöÄ
