services:
  # SGLang - Brain (logic/routing)
  # MXFP4 quantization preserves reasoning logic while reducing memory footprint
  # Lowered mem-fraction to ensure 24GB headroom for OS/ArangoDB/Qdrant
  cortex-brain:
    image: ${CORTEX_IMAGE}
    command: python3 -m sglang.launch_server --model-path ${BRAIN_MODEL_PATH} --host 0.0.0.0 --port 30000 --tp-size 2 --quantization mxfp4 --mem-fraction-static 0.70 --max-running-requests 2
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
      - HF_HOME=/raid/vyasa/hf_cache
      - TRANSFORMERS_CACHE=/raid/vyasa/model_cache
    cpuset: "10-19"
    shm_size: "16gb"
    mem_limit: "64g"
    ports:
      - "${PORT_BRAIN}:30000"
    restart: unless-stopped
    volumes:
      - /raid/vyasa/hf_cache:/raid/vyasa/hf_cache
      - /raid/vyasa/model_cache:/root/.cache/huggingface
    networks:
      - vyasa-net
    healthcheck:
      test: ["CMD", "bash", "-c", "exec 3<>/dev/tcp/localhost/30000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # SGLang - Worker (general tasks)
  # FP4 quantization maximizes memory efficiency for extraction tasks
  # Increased KV cache capacity due to reduced weight size
  # Lowered mem-fraction to ensure 24GB headroom for OS/ArangoDB/Qdrant
  cortex-worker:
    image: ${CORTEX_IMAGE}
    command: python3 -m sglang.launch_server --model-path ${WORKER_MODEL_PATH} --host 0.0.0.0 --port 30001 --tp-size 1 --quantization fp4 --mem-fraction-static 0.70 --max-running-requests 4 --context-length 16384
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=1
      - HF_HOME=/raid/vyasa/hf_cache
      - TRANSFORMERS_CACHE=/raid/vyasa/model_cache
    cpuset: "10-19"
    shm_size: "16gb"
    mem_limit: "32g"
    ports:
      - "${PORT_WORKER}:30001"
    restart: unless-stopped
    volumes:
      - /raid/vyasa/hf_cache:/raid/vyasa/hf_cache
      - /raid/vyasa/model_cache:/root/.cache/huggingface
    networks:
      - vyasa-net
    healthcheck:
      test: ["CMD", "bash", "-c", "exec 3<>/dev/tcp/localhost/30001"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # SGLang - Vision
  # INT8 quantization for vision models (balanced precision/performance)
  # Memory guardrails: bounded footprint to prevent UMA spikes
  cortex-vision:
    image: ${CORTEX_IMAGE}
    command: python3 -m sglang.launch_server --model-path ${VISION_MODEL_PATH} --host 0.0.0.0 --port 30002 --tp-size 2 --quantization int8 --mem-fraction-static 0.75 --max-running-requests 2
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=${VISION_GPU_IDS}
      - HF_HOME=/raid/vyasa/hf_cache
      - TRANSFORMERS_CACHE=/raid/vyasa/model_cache
    cpuset: "10-19"
    shm_size: "16gb"
    ports:
      - "${PORT_VISION}:30002"
    restart: unless-stopped
    volumes:
      - /raid/vyasa/hf_cache:/raid/vyasa/hf_cache
      - /raid/vyasa/model_cache:/root/.cache/huggingface
    networks:
      - vyasa-net
    healthcheck:
      test: ["CMD", "bash", "-c", "exec 3<>/dev/tcp/localhost/30002"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # Ollama - Drafter
  # Pinned to efficiency cores for management tasks
  drafter:
    image: ${DRAFTER_IMAGE}
    environment:
      - CUDA_VISIBLE_DEVICES=0
    cpuset: "0-9"
    shm_size: "16gb"
    ports:
      - "${PORT_DRAFTER}:11435"
    volumes:
      - /raid/vyasa/ollama:/root/.ollama
    restart: unless-stopped
    networks:
      - vyasa-net
    healthcheck:
      test: ["CMD", "bash", "-c", "exec 3<>/dev/tcp/localhost/11435"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  # ArangoDB - Graph
  graph:
    image: ${MEMORY_IMAGE}
    cpuset: "0-9"
    ports:
      - "${PORT_MEMORY}:${PORT_MEMORY}"
    environment:
      - ARANGO_ROOT_PASSWORD=${ARANGO_ROOT_PASSWORD}
    volumes:
      - /raid/vyasa/arangodb:/var/lib/arangodb3
      - ./scripts/arango-init.js:/docker-entrypoint-initdb.d/arango-init.js:ro
    networks:
      - vyasa-net
    healthcheck:
      test: ["CMD", "bash", "-c", "exec 3<>/dev/tcp/localhost/${PORT_MEMORY}"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 20s

  # Qdrant - Vector DB
  vector:
    image: ${VECTOR_IMAGE}
    cpuset: "0-9"
    ports:
      - "${PORT_VECTOR}:${PORT_VECTOR}"
    environment:
      - QDRANT_API_KEY=${QDRANT_API_KEY}
    volumes:
      - /raid/vyasa/qdrant:/qdrant/storage
      - ./scripts/qdrant-init.sh:/scripts/qdrant-init.sh:ro
    entrypoint: ["/bin/sh", "-c"]
    command: |
      /qdrant/qdrant &
      PID=$$!
      sleep 5
      /bin/sh /scripts/qdrant-init.sh
      wait $$PID
    networks:
      - vyasa-net
    healthcheck:
      test: ["CMD", "bash", "-c", "exec 3<>/dev/tcp/localhost/${PORT_VECTOR}"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 20s

  # Next.js Console
  console:
    build:
      context: ../src/console
      dockerfile: ${CONSOLE_DOCKERFILE:-Dockerfile}
    cpuset: "0-9"
    ports:
      - "${PORT_CONSOLE}:${PORT_CONSOLE}"
    volumes:
      # Mount deploy directory for accessing configuration files (forbidden_vocab.yaml, etc.)
      - ./deploy:/app/deploy:ro
    environment:
      - NODE_ENV=${CONSOLE_NODE_ENV:-${NODE_ENV:-production}}
      - NEXT_PUBLIC_API_URL=http://localhost:${PORT_BRAIN}/v1
      - CORTEX_SERVICE_URL=http://cortex-brain:30000
      - MEMORY_SERVICE_URL=http://graph:${PORT_MEMORY}
      - SENTENCE_TRANSFORMER_URL=http://embedder:30010
      - QDRANT_URL=http://vector:${PORT_VECTOR}
      - ARANGODB_DB=${GRAPH_DB_NAME}
      - ARANGODB_USER=${ARANGODB_USER}
      - ARANGO_ROOT_PASSWORD=${ARANGO_ROOT_PASSWORD}
      - QDRANT_API_KEY=${QDRANT_API_KEY}
      - CONSOLE_PASSWORD=${CONSOLE_PASSWORD}
      - NEXTAUTH_SECRET=${CONSOLE_SECRET}
      - NEXTAUTH_URL=http://localhost:${PORT_CONSOLE}
      - AUTH_TRUST_HOST=true
    depends_on:
      - cortex-brain
      - graph
      - vector
    networks:
      - vyasa-net

  # Embedder
  # Pinned to efficiency cores for management tasks
  embedder:
    build:
      context: ../src/embedder
    cpuset: "0-9"
    shm_size: "16gb"
    ports:
      - "${PORT_EMBEDDER}:30010"
    environment:
      - EMBEDDING_MODEL_PATH=${EMBEDDING_MODEL_PATH:-BAAI/bge-large-en-v1.5}
      - HF_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=1
      - HF_HOME=/raid/vyasa/hf_cache
      - TRANSFORMERS_CACHE=/raid/vyasa/model_cache
    volumes:
      - /raid/vyasa/hf_cache:/raid/vyasa/hf_cache
      - /raid/vyasa/model_cache:/root/.cache/huggingface
    restart: unless-stopped
    networks:
      - vyasa-net
    healthcheck:
      test: ["CMD", "bash", "-c", "exec 3<>/dev/tcp/localhost/30010"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Orchestrator (agentic workflow API)
  orchestrator:
    image: ${ORCHESTRATOR_IMAGE}
    working_dir: /app
    # Install system dependencies and Python packages, then run uvicorn
    # Note: torch_memory_saver requires CUDA headers to build, but orchestrator only uses SGLang as HTTP client
    # Solution: Use setup script to create minimal CUDA header stubs to satisfy build requirements
    # The orchestrator doesn't actually use CUDA at runtime - it just makes HTTP requests to cortex services
    command: ["/app/deploy/scripts/orchestrator-setup.sh"]
    cpuset: "0-9"
    ports:
      - "8000:8000"
    volumes:
      - ../src:/app
      - ../requirements.txt:/app/requirements.txt:ro
      - ./scripts:/app/deploy/scripts:ro
      - ./scripts/orchestrator-setup.sh:/app/deploy/scripts/orchestrator-setup.sh:ro
      - /raid/vyasa/telemetry:/raid/telemetry
      - /raid/vyasa/scratch:/raid/scratch
    environment:
      - BRAIN_URL=http://cortex-brain:30000
      - WORKER_URL=http://cortex-worker:30001
      - VISION_URL=http://cortex-vision:30002
      - DRAFTER_URL=http://drafter:11435
      - MEMORY_URL=http://graph:${PORT_MEMORY}
      - VECTOR_URL=http://vector:${PORT_VECTOR}
      - ARANGODB_DB=${GRAPH_DB_NAME}
      - ARANGODB_USER=${ARANGODB_USER}
      - ARANGODB_PASSWORD=${ARANGO_ROOT_PASSWORD}
      - QDRANT_API_KEY=${QDRANT_API_KEY}
      - LOG_FORMAT=json
    depends_on:
      - cortex-brain
      - cortex-worker
      - cortex-vision
      - drafter
      - graph
      - vector
    networks:
      - vyasa-net
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s=socket.socket(); s.settimeout(2); s.connect(('localhost',8000)); s.close()"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

networks:
  vyasa-net:
    name: vyasa-net
    external: true

volumes:
  memory_data:
  qdrant_data:
  ollama_data:
